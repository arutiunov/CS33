Name: Artiom Arutiunov
ID: 504597668
Professor Eggert
Lab 4: OpenMP Lab


The first step I took in completing this lab was to download the tar file. After doing so and
unsuccessfully attempting to compile an executable file, I realized that my gcc version may
be outdated. As a result, I had to transfer the downloaded “openmplab” folder on my desktop
to my SEASnet account. I did so by using the command:

	scp -r *source* classaru@lnxsrv.seas.ucla.edu:*directory

After reaching the folder on my SEASnet desktop and ensuring that all the contents were there
using the command ‘ls’, I generated an executable file called ‘seq’ which did not have
the OpenMP lab enabled:

	[classaru@lnxsrv09 ~/Desktop/openmplab]$ make seq
	gcc -o seq  -O3 filter.c main.c func.c util.c -lm
	[classaru@lnxsrv09 ~/Desktop/openmplab]$ ./seq
	FUNC TIME : 0.483444
	TOTAL TIME : 2.267392

Since the goal of the project was to modify the five functions with the func.c file, I
focused on the ‘FUNC TIME’ value returned by the function. As seen above, it took roughly
48 seconds for the function to be completed without OpenMP. It should be noted that this
generally fluctuated roughly between 0.44 and 0.5, assuming that server load did not
significantly slow down the ‘FUNC TIME’ to 0.8 or even 0.9 seconds, as it did several
times over the course of working on the lab. 

Next, I used the GPROF tool to pinpoint which functions within func.c were bottlenecks.
I ran the following commands to generate the GPROF profile of the executable ‘seq’,
which does not enable OpenMP (and is thus the original version of func.c):

 	[classaru@lnxsrv09 ~/Desktop/openmplab]$ make seq GPROF=1
	gcc -o seq  -O2 -pg filter.c main.c func.c util.c -lm
	[classaru@lnxsrv09 ~/Desktop/openmplab]$ ./seq
	FUNC TIME : 0.545965
	TOTAL TIME : 2.370656
	[classaru@lnxsrv09 ~/Desktop/openmplab]$ gprof seq | less

It should be noted that according to OpenMP documentation I found, the overall time
and function times are slowed down by roughly 25%, which explains the difference
between the original time I ran ‘./seq’ and this time. The last command in the block above
generated the following (and abridged) GPROF profile of ‘seq’:


Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 62.80      0.37     0.37       15    24.70    26.09  func1
 18.67      0.48     0.11  5177344     0.00     0.00  rand2
  5.09      0.51     0.03        1    30.04   119.28  addSeed
  3.39      0.53     0.02   491520     0.00     0.00  findIndexBin
  3.39      0.55     0.02                             sequence
  1.70      0.56     0.01       15     0.67     0.67  func3
  1.70      0.57     0.01       15     0.67     2.00  func5
  1.70      0.58     0.01        1    10.01    10.01  imdilateDisk
  1.70      0.59     0.01                             filter
  0.00      0.59     0.00   983042     0.00     0.00  round
  0.00      0.59     0.00       16     0.00     0.00  dilateMatrix
  0.00      0.59     0.00       15     0.00     0.00  func2
  0.00      0.59     0.00       15     0.00     0.00  func4
  0.00      0.59     0.00       15     0.00     0.00  rand1
  0.00      0.59     0.00        2     0.00     0.00  get_time
  0.00      0.59     0.00        1     0.00     0.00  elapsed_time
  0.00      0.59     0.00        1     0.00     0.00  fillMatrix
  0.00      0.59     0.00        1     0.00     0.00  func0
  0.00      0.59     0.00        1     0.00     0.00  getNeighbors

 % the percentage of the total running time of the time program used by this function.

Looking at this profile, I see that func1 is a blatant bottleneck. It takes up
over 62% of the total time, which means that in working on func.c, the priority
should be to modify and improve the performance of func1. However, func3 and func5 also 
took up a sizable chunk of time, so I decided to try to parallelize each program. 
Thus, my next step was to use the emacs editor to enter func.c and edit its code. 

Prior to considering any other optimization methods besides OpenMP, I tried to
apply as many OpenMP directories as I could, function by function. In particular
I used three different types of OpenMP directories in speeding up my code:

	- #pragma omp parallel for
	- #pragma omp parallel for private(__)
	- #pragma mop parallel for reduction(+:__)

I used “pragma omp parallel for” in cases when the code inside the loop
can be parallelized, but there is no accumulation occurring and no
variables outside the scope of the loop (excluding parameter variables).

I used “pragma omp parallel for private(_)” in cases where there is a
variable being used inside the loop that should be privatized for
each thread, in effect making sure that the code is parallelized
without compromising the result of the calculation.

I used “#pragma mop parallel for reduction(+:__)” in cases where
there is a accumulator, in order to ensure that the multiple threads
correctly add the result to the accumulator.



Below are my observations and changes to the functions:

	func0:

	void func0(double *weights, double *arrayX, double *arrayY, int xr, int yr, int n)
	{
		int i;
    		#pragma omp parallel for <— no accumulator, i assigned value within loop
		for(i = 0; i < n; i++){
			weights[i] = 1/((double)(n));
			arrayX[i] = xr;
			arrayY[i] = yr;
		}
	}

	func1:

	void func1(int *seed, int *array, double *arrayX, double *arrayY,
			double *probability, double *objxy, int *index,
			int Ones, int iter, int X, int Y, int Z, int n)
	{
		int i, j;
   		int index_X, index_Y;
		int max_size = X*Y*Z;

   		#pragma omp parallel for <— no accumulator, i assigned value within loop
   		for(i = 0; i < n; i++){
   			arrayX[i] += 1 + 5*rand2(seed, i);
   			arrayY[i] += -2 + 2*rand2(seed, i);
   		}

   		#pragma omp parallel for private(index_X, index_Y, j) <— these 3 variables have to
								be privatized because they are not defined within loop
   		for(i = 0; i<n; i++){
   			for(j = 0; j < Ones; j++){
   				index_X = round(arrayX[i]) + objxy[j*2 + 1];
   				index_Y = round(arrayY[i]) + objxy[j*2];
   				index[i*Ones + j] = fabs(index_X*Y*Z + index_Y*Z + iter);
   				if(index[i*Ones + j] >= max_size)
   					index[i*Ones + j] = 0;
   			}
   		probability[i] = 0;

   		for(j = 0; j < Ones; j++) {
   			probability[i] += (pow((array[index[i*Ones + j]] - 100),2) -
   							  pow((array[index[i*Ones + j]]-228),2))/50.0;
   		}
   		probability[i] = probability[i]/((double) Ones);
   		}
	}

	func2:

	void func2(double *weights, double *probability, int n)
	{
		int i;
		double sumWeights=0;
		#pragma omp parallel for <— no accumulation, i defined within loop
		for(i = 0; i < n; i++)
   			weights[i] = weights[i] * exp(probability[i]);
		#pragma omp parallel for reduction(+:sumWeights) <— accumulation of sumWeights
   		for(i = 0; i < n; i++)
   			sumWeights += weights[i];
		#pragma omp parallel for <— no accumulation, i defined within loop
		for(i = 0; i < n; i++)
   			weights[i] = weights[i]/sumWeights;
	}

	func3:

	void func3(double *arrayX, double *arrayY, double *weights, double *x_e, double *y_e, int n)
	{
		double estimate_x=0.0;
		double estimate_y=0.0;
    		int i;
		#pragma mop parallel for reduction(+:estimate_x, estimate_y) <— accumulation of estimate_x and estimate_y	
		for(i = 0; i < n; i++){
   			estimate_x += arrayX[i] * weights[i];
   			estimate_y += arrayY[i] * weights[i];
   		}

		*x_e = estimate_x;
		*y_e = estimate_y;
	}

	func4:

	void func4(double *u, double u1, int n)
	{
		int i;
		#pragma omp parallel for <— no accumulation, i defined within for loop
		for(i = 0; i < n; i++){
   			u[i] = u1 + i/((double)(n));
   		}
	}

	func5:

	void func5(double *x_j, double *y_j, double *arrayX, double *arrayY, double *weights, double *cfd, double *u, int n)
	{
		int i, j;
		#pragma mop parallel for private(i) <— i is not defined within the loop
		for(j = 0; j < n; j++){
   			//i = findIndex(cfd, n, u[j]);
   			i = findIndexBin(cfd, 0, n, u[j]);
   			if(i == -1)
   			i = n-1;
   			x_j[j] = arrayX[i];
   			y_j[j] = arrayY[i];
   		}
		#pragma omp parallel for <— no accumulation, i is defined within the loop
		for(i = 0; i < n; i++){
			arrayX[i] = x_j[i];
			arrayY[i] = y_j[i];
			weights[i] = 1/((double)(n));
		}
	}

Next, I created an executable file called ‘omp’ using the following commands:

[classaru@lnxsrv09 ~/Desktop/openmplab]$ make omp
gcc -o omp  -O3 -fopenmp filter.c main.c func.c util.c -lm
[classaru@lnxsrv09 ~/Desktop/openmplab]$ ./omp
FUNC TIME : 0.034456
TOTAL TIME : 1.818608

As seen, I increased FUNC TIME by (0.483444/0.034456) over 14 times! To confirm
that I actually increased the speed of the functions in func.c (and in effect
removed the amount of time they take up in the execution of the program), I ran
another GPROF profile, this time on the ‘mop’ executable: 


[classaru@lnxsrv09 ~/Desktop/openmplab]$ make omp GPROF=1
gcc -o omp  -O2 -pg -fopenmp filter.c main.c func.c util.c -lm
[classaru@lnxsrv09 ~/Desktop/openmplab]$ ./omp
FUNC TIME : 0.125125
TOTAL TIME : 1.962801
[classaru@lnxsrv09 ~/Desktop/openmplab]$ gprof omp | less


Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 69.64      0.48     0.48                             filter
 20.31      0.62     0.14  4252921     0.00     0.00  rand2
  5.80      0.66     0.04    26120     0.00     0.00  findIndexBin
  2.90      0.68     0.02                             sequence
  1.45      0.69     0.01        1    10.01    10.01  imdilateDisk
  0.00      0.69     0.00    78605     0.00     0.00  round
  0.00      0.69     0.00       16     0.00     0.00  dilateMatrix
  0.00      0.69     0.00       15     0.00     0.00  func1
  0.00      0.69     0.00       15     0.00     0.00  func2
  0.00      0.69     0.00       15     0.00     0.00  func3
  0.00      0.69     0.00       15     0.00     0.00  func4
  0.00      0.69     0.00       15     0.00     0.00  func5
  0.00      0.69     0.00       15     0.00     0.00  rand1
  0.00      0.69     0.00        2     0.00     0.00  get_time
  0.00      0.69     0.00        1     0.00   138.22  addSeed
  0.00      0.69     0.00        1     0.00     0.00  elapsed_time
  0.00      0.69     0.00        1     0.00     0.00  fillMatrix
  0.00      0.69     0.00        1     0.00     0.00  func0
  0.00      0.69     0.00        1     0.00     0.00  getNeighbors

 % the percentage of the total running time of the time program used by this function.


As seen, the amount of time the functions take up now is very small, meaning that the
OpenMP parallelism directories were actually effective.

Lastly, I needed to check whether the output was still correct. I continued to use the
same ‘omp’ executable to check that the program’s output was still correct:

[classaru@lnxsrv09 ~/Desktop/openmplab]$ make omp MTRACE=1
gcc -o omp  -O3 -DMTRACE -fopenmp filter.c main.c func.c util.c -lm
[classaru@lnxsrv09 ~/Desktop/openmplab]$ ./omp
FUNC TIME : 0.047697
TOTAL TIME : 1.873731
[classaru@lnxsrv09 ~/Desktop/openmplab]$ make checkmem
mtrace filter mtrace.out || true

The command spits out an output saying ‘Memory not freed:’, but according to the instructors, 
there is a bug in OpenMP so that memory leaks on part of OpenMP threading is not a concern.

Nevertheless, there was one final obstacle in the way of finishing this project. The problem
rested in the fact that I had all along been working lnxsrv09, which produced the results
seen above. However, no matter how many times I tested my code and no matter at what time
I tested the code, I would consistently get between 0.8 and 0.9 seconds for func.c with
OpenMP enabled (mop executable). This was a big problem as the actual lab report was
tested on lnxrv07, which would consistently produce much slower results that what was
expected. 

As such, I once more took to the Internet to try to solve my problem. After fiddling around
with the OpenMP directories, I came to a suggestion that worked: specifying the number
of threads that should be used to parallelize the code. This is done by using the
‘num_thread(N amount of threads)’ immediately after the ‘for’ within the OpenMP directory,
as such:
	#pragma omp parallel for num_threads(N)

After playing around with the value of N in order to optimize the speed of func.c, I
concluded that the value of 28 was the best. I subsequently went through each
directory and specified the number of threads to be 28. 

Finally, I went back and executed ‘seq’ one more time, whose time still floated
around 0.45. After specifying the number of threads, the ‘omp’ executable
clocked in between 0.03 and 0.04 consistently (after running the executable
more than 30 times). As far as I understand, the reason for this inconcistency
between the two servers is that since everyone is using lnxsrv07 to test their code, 
there is a slowdown in the performance of the server, which, coupled with the
default number of threads (32), resulted in slow performance. Just to be sure,
I also ran the checks for correct output and memory leaks (despite no longer
being accounted for) to be sure that my newly modified version of func.c still
behaved properly.